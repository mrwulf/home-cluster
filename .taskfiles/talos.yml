---
version: "3"

vars:
  CLUSTER_NAME:
    sh: yq '.clusterName' talos/talconfig.yaml
  NODE_LIST:
    sh: yq '.nodes[].hostname' talos/talconfig.yaml | tr '\n' ',' | head -c -1
  ONE_NODE:
    sh: echo '{{.NODE_LIST}}' | tr ',' '\n' | shuf | head -n1
  MASTER_LIST:
    sh: yq '.nodes[] | select(.controlPlane==true) | .hostname ' talos/talconfig.yaml | tr '\n' ',' | head -c -1
  WORKER_LIST:
    sh: yq '.nodes[] | select(.controlPlane!=true) | .hostname ' talos/talconfig.yaml | tr '\n' ',' | head -c -1
  NODE_LIST_MASTERSFIRST: "{{.MASTER_LIST}},{{.WORKER_LIST}}"
  ONE_MASTER:
    sh: echo '{{.MASTER_LIST}}' | tr ',' '\n' | shuf | head -n1

tasks:
  list:
    desc: List all the hosts
    silent: true
    cmds:
      - |
        echo {{.NODE_LIST}}

  list-masters:
    desc: List all the masters
    silent: true
    cmds:
      - |
        echo {{.MASTER_LIST}}

  list-workers:
    desc: List all the workers
    silent: true
    cmds:
      - |
        echo {{.WORKER_LIST}}

  list-nodes:
    desc: List all the nodes
    silent: true
    cmds:
      - |
        echo {{.NODE_LIST_MASTERSFIRST}}

  get-a-master:
    silent: true
    cmds:
      - echo {{.ONE_MASTER}}

  get-a-node:
    silent: true
    cmds:
      - echo {{.ONE_NODE}}

  debug-node:
    desc: Create a privileged container on a node for debugging, ex. task talos:debug-node -- <node-name>
    interactive: true
    silent: true
    vars:
      NODE: '{{.CLI_ARGS | default .ONE_MASTER}}'
    cmds:
      - kubectl debug node/{{.NODE}} -it --image=fedora:36 -- bash

  dashboard:
    desc: Show the Talos dashboard with all the hosts
    silent: true
    cmds:
      - |
        talosctl -n {{.NODE_LIST}} dashboard

  ctl-all:
    desc: Talosctl with all the hosts
    silent: true
    cmds:
      - |
        talosctl -n {{.NODE_LIST}} {{.CLI_ARGS | default "--help"}}

  ctl-one:
    desc: Talosctl with a single host
    silent: true
    cmds:
      - |
        talosctl -n {{.ONE_MASTER}} {{.CLI_ARGS | default "--help"}}

  kubelet-uptime:
    desc: Show kubelet uptime for each node
    silent: true
    cmds:
      - task: ctl-all
        vars:
          CLI_ARGS: 'service | tail -n +2 | grep kubelet | sort -r -k 5'

  theila:
    desc: Show the Theila web-ui dashboard with all the hosts
    silent: true
    cmds:
      - |
        docker pull ghcr.io/siderolabs/theila:latest
        xdg-open http://localhost:8080/ >/dev/null 2>&1
        docker run --rm -it --volume ${HOME}/.talos/config:/opt/talosconfig:ro --env TALOSCONFIG=/opt/talosconfig \
                            --publish 8080:8080 ghcr.io/siderolabs/theila:latest --address 0.0.0.0 >/dev/null 2>&1
  generate-secrets:
    dir: "{{.PROJECT_DIR}}"
    vars:
      FILENAME: "{{.PROJECT_DIR}}/talos/talsecret.sops.yaml"
    cmds:
      - talhelper gensecret > {{.FILENAME}}
      - sops -e -i {{.FILENAME}}
    preconditions:
      - sh: "[ ! -f {{.FILENAME}} ]"
        msg: "Talos secrets already exist!"
      - sh: "[ -n $SOPS_AGE_KEY_FILE ]"
        msg: "Sops not configured yet!"

  generate-configs:
    desc: Use talhelper to regenerate individual node configs
    silent: true
    dir: "{{.PROJECT_DIR}}/talos/"
    sources:
      - "{{.PROJECT_DIR}}/talos/talconfig.yaml"
      - "{{.PROJECT_DIR}}/talos/talsecret.sops.yaml"
    generates:
      - "{{.PROJECT_DIR}}/talos/clusterconfig/*"
      - "{{.PROJECT_DIR}}/talos/clusterconfig/talosconfig"
      - "{{.PROJECT_DIR}}/talos/clusterconfig/.gitignore"
    cmds:
      - talhelper genconfig

  diff-config:
    desc: Diff config against running node config
    ignore_error: true
    silent: true
    requires:
      vars: ["NODE", "CLUSTER_NAME"]
    cmds:
      - |
        talosctl apply-config --dry-run -n {{.NODE}} -f {{.PROJECT_DIR}}/talos/clusterconfig/{{.CLUSTER_NAME}}-{{.NODE}}.yaml

  diff-all-configs:
    desc: Diff config for all nodes
    silent: true
    cmds:
      - for: { var: NODE_LIST, split: ",", as: NODE }
        task: diff-config
        vars:
          NODE: '{{.NODE}}'

  apply-config:
    desc: Apply config against running node config
    ignore_error: true
    silent: true
    vars:
      MODE: '{{.MODE | default "no-reboot"}}'
    requires:
      vars: ["NODE", "CLUSTER_NAME"]
    deps:
      - task: generate-configs
    cmds:
      - |
        talosctl apply-config -n {{.NODE}} -f {{.PROJECT_DIR}}/talos/clusterconfig/{{.CLUSTER_NAME}}-{{.NODE}}.yaml -m={{.MODE}}

  apply-all-configs:
    desc: Apply config for all nodes
    silent: true
    cmds:
      - for: { var: NODE_LIST, split: ",", as: NODE }
        task: apply-config
        vars:
          NODE: '{{.NODE}}'

  upgrade:
    desc: Upgrade Talos on a single node to the version in talconfig.yaml
    silent: true
    vars:
      NODE_NAME:
        sh: WHOLE={{.NODE}} && echo "${WHOLE%.home}"
      CURRENT_VERSION:
        sh: kubectl get node "{{.NODE_NAME}}" --output jsonpath='{.status.nodeInfo.osImage}' | grep -oP 'v[0-9]+.[0-9]+.[0-9]+'
      TARGET_VERSION:
        sh: yq '.talosVersion' talos/talconfig.yaml
      TARGET_VERSION_IMAGE: "ghcr.io/siderolabs/installer:{{.TARGET_VERSION}}"
    requires:
      vars: ["NODE"]
    deps:
      - task: generate-configs
      - task: diff-config
        vars:
          NODE: "{{.NODE}}"
      - task: apply-config
        vars:
          NODE: "{{.NODE}}"
    cmds:
      - |
        echo "Current Version: {{.CURRENT_VERSION}}"
        echo "Target Version: {{.TARGET_VERSION}}"
        echo "Target Version Image: {{.TARGET_VERSION_IMAGE}}"
        echo "Target Node: {{.NODE}}"
        if [ "{{.CURRENT_VERSION}}" == "{{.TARGET_VERSION}}" ]; then
            echo "Talos is already up to date on version '{{.CURRENT_VERSION}}', skipping upgrade ..."
        else
            talosctl upgrade -n {{.NODE}} --image {{.TARGET_VERSION_IMAGE}} --preserve=true
        fi
        #
        echo "Waiting for Talos to be healthy on node '{{.NODE}}' ..."
        until talosctl --nodes {{.NODE}} health --wait-timeout=10m --server=false
        do
            echo "Waiting for talos health to be OK on node '{{.NODE}}' ..."
            sleep 10
        done
        #
        echo "Waiting for Ceph health to be OK on node '{{.NODE}}' ..."
        until kubectl wait --timeout=5m \
            --for=jsonpath=.status.ceph.health=HEALTH_OK cephcluster \
                --all --all-namespaces;
        do
            echo "Waiting for Ceph health to be OK on node '{{.NODE}}' ..."
            sleep 10
        done

  upgrade-all:
    desc: Upgrade Talos on all the nodes to the version in talconfig.yaml
    silent: true
    cmds:
      - for: { var: NODE_LIST_MASTERSFIRST, split: ",", as: NODE }
        task: upgrade
        vars:
          NODE: '{{.NODE}}'
      - task: :cluster:cleanup
      - kubectl -n kube-system rollout restart daemonset cilium

  upgrade-k8s-precheck:
    requires: 
      vars: [ TARGET_VERSION ]
    cmds:
      - echo "Running a dry-run upgrade..."
      - talosctl -n {{.ONE_MASTER}} upgrade-k8s --to={{.TARGET_VERSION}} --dry-run

  upgrade-k8s:
    vars:
      TARGET_VERSION:
        sh: yq '.kubernetesVersion' talos/talconfig.yaml
    deps: 
      - task: upgrade-k8s-precheck
        vars: 
          TARGET_VERSION: "{{.TARGET_VERSION}}"
        silent: false
    prompt: 'Are you sure you want to upgrade kubernetes?'
    cmds:
      - echo "Running the real upgrade..."
      - talosctl -n {{.ONE_MASTER}} upgrade-k8s --to={{.TARGET_VERSION}}

  refresh-certificates:
    # https://www.talos.dev/v1.3/talos-guides/configuration/managing-pki/
    desc: Renew Administrator Certificate
    silent: true
    deps:
      - task: generate-configs
    vars:
      FILE: '{{.TALOSCONFIG | default "$HOME/.talos/config"}}'
    cmds:
      - |
        yq -i '. *= load("{{.PROJECT_DIR}}//talos/clusterconfig/talosconfig")' {{.FILE}}
        talosctl kubeconfig -n {{.ONE_MASTER}} --force

  reboot:
    desc: Safely restart a talos node
    silent: true
    vars:
      NODE_NAME:
        sh: WHOLE={{.NODE}} && echo "${WHOLE%.home}"
    requires:
      vars: ["NODE"]
    deps:
      - task: generate-configs
      - task: diff-config
        vars:
          NODE: "{{.NODE}}"
      - task: apply-config
        vars:
          NODE: "{{.NODE}}"
    cmds:
      - |
        echo "Target Node: {{.NODE}}"
        talosctl reboot -n {{.NODE}} --mode=powercycle
        #
        echo "Waiting for Talos to be healthy on node '{{.NODE}}' ..."
        until talosctl --nodes {{.NODE}} health --wait-timeout=10m --server=false
        do
            echo "Waiting for talos health to be OK on node '{{.NODE}}' ..."
            sleep 10
        done
        #
        echo "Waiting for Ceph health to be OK on node '{{.NODE}}' ..."
        until kubectl wait --timeout=5m \
            --for=jsonpath=.status.ceph.health=HEALTH_OK cephcluster \
                --all --all-namespaces;
        do
            echo "Waiting for Ceph health to be OK on node '{{.NODE}}' ..."
            sleep 10
        done

  reboot-all:
    desc: Reboot Talos on all the nodes
    silent: true
    cmds:
      - for: { var: NODE_LIST_MASTERSFIRST, split: ",", as: NODE }
        task: reboot
        vars:
          NODE: '{{.NODE}}'
      - task: :cluster:cleanup
      - kubectl -n kube-system rollout restart daemonset cilium
