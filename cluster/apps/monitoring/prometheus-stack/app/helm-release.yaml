---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: &app kube-prometheus-stack
spec:
  interval: 30m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 72.4.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
  values:
    kubeApiServer:
      enabled: true
      serviceMonitor:
        selector:
          k8s-app: kube-apiserver
        metricRelabelings:
          # Drop high cardinality labels by onedr0p
          - action: drop
            sourceLabels: ["__name__"]
            regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
          - action: drop
            sourceLabels: ["__name__"]
            regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
    kubeEtcd:
      enabled: true
      endpoints:
        - 10.0.1.51
        - 10.0.1.52
        - 10.0.1.53
      service:
        enabled: true
        port: 2381
        targetPort: 2381
      serviceMonitor:
        scheme: http
    kubeControllerManager:
      enabled: true
      serviceMonitor:
        enabled: true
        scheme: https
        selector:
          k8s-app: kube-controller-manager
    kubeScheduler:
      enabled: true
      serviceMonitor:
        enabled: true
        scheme: https
        selector:
          k8s-app: kube-scheduler
    kubeProxy:
      enabled: false
    coreDns:
      enabled: true
      serviceMonitor:
        enabled: true
        selector:
          k8s-app: kube-dns
    kubelet:
      enabled: true
      endpoints:
        - 10.0.1.51
        - 10.0.1.52
        - 10.0.1.53
      service:
        enabled: true
        port: 10250
        targetPort: 10250
    grafana:
      assertNoLeakedSecrets: false
      adminPassword: ${GRAFANA_PASS}
      defaultDashboardsTimezone: browser
      deploymentStrategy:
        type: Recreate
      plugins:
        - grafana-piechart-panel
      serviceMonitor:
        enabled: true
      persistence:
        type: pvc
        enabled: true
        storageClassName: ceph-block
        accessModes:
          - ReadWriteOnce
        size: 2Gi
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Grafana"
        hosts:
          - &host "grafana.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host
      grafana.ini:
        server:
          root_url: "https://grafana.home.${SECRET_DOMAIN}"
        analytics:
          check_for_updates: false
        security:
          allow_embedding: true
        auth:
          signout_redirect_url: "https://auth.home.${SECRET_DOMAIN}/application/o/grafana/end-session/"
          # Switch off auto_login to be able to log in as admin to solve https://github.com/grafana/grafana/issues/29211
          oauth_auto_login: false
        auth.generic_oauth:
          enabled: true
          name: "Authentik"
          client_id: "${GRAFANA_OAUTH_CLIENTID}"
          client_secret: "${GRAFANA_OAUTH_SECRET}"
          scopes: "openid profile email"
          auth_url: "https://auth.${SECRET_DOMAIN}/application/o/authorize/"
          token_url: "https://auth.${SECRET_DOMAIN}/application/o/token/"
          api_url: "https://auth.${SECRET_DOMAIN}/application/o/userinfo/"
          allow_sign_up: true
          email_attribute_name: email

        auth.generic_oauth.group_mapping:
          role_attribute_path: |
            contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'people') && 'Viewer'
          org_id: 1
        auth.basic:
          enabled: false
          # disable_login_form: false
        auth.anonymous:
          enabled: false

      datasources:
        datasource.yaml:
          apiVersion: 1
          deleteDatasources:
            - name: Loki
              orgId: 1
            - name: Alertmanager
              orgId: 1
            - name: Prometheus
              orgId: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://kube-prometheus-stack-prometheus.monitoring:9090
              isDefault: true
              uid: Prometheus
            - name: Loki
              type: loki
              access: proxy
              url: http://loki-stack:3100
            - name: Alertmanager
              type: alertmanager
              access: proxy
              url: http://kube-prometheus-stack-alertmanager.monitoring:9093
              jsonData:
                implementation: prometheus
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: "default"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
            - name: "unifi"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/unifi
            - name: "ceph"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/ceph
            - name: "kubernetes"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/kubernetes
      dashboards:
        default:
          spegel:
            # renovate: depName="Spegel"
            gnetId: 18089
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          cloudflared:
            # renovate: depName="Cloudflare Tunnels (cloudflared)"
            gnetId: 17457 # https://grafana.com/grafana/dashboards/17457?tab=revisions
            revision: 6
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/13502-minio-dashboard/
          minio:
            gnetId: 13502
            revision: 19
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/1860-node-exporter-full/
          node-exporter-full:
            gnetId: 1860
            revision: 29
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/763-redis-dashboard-for-prometheus-redis-exporter-1-x/
          redis:
            gnetId: 763
            revision: 4
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/5342-ceph-pools/
          cert-manager:
            url: https://raw.githubusercontent.com/monitoring-mixins/website/refs/heads/master/assets/cert-manager/dashboards/overview.json
            datasource: Prometheus
          flux-cluster:
            url: https://raw.githubusercontent.com/fluxcd/flux2-monitoring-example/refs/heads/main/monitoring/configs/dashboards/cluster.json
            datasource: Prometheus
          flux-control-plane:
            url: https://raw.githubusercontent.com/fluxcd/flux2-monitoring-example/refs/heads/main/monitoring/configs/dashboards/control-plane.json
            datasource: Prometheus
          flux-logs:
            url: https://raw.githubusercontent.com/fluxcd/flux2-monitoring-example/refs/heads/main/monitoring/configs/dashboards/logs.json
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/21356-volsync-dashboard/
          volsync:
            gnetId: 21356
            revision: 3
            datasource: Prometheus
        unifi:
          unifi-insights:
            # renovate: depName="UniFi-Poller: Client Insights - Prometheus"
            gnetId: 11315
            revision: 9
            datasource: Prometheus
          unifi-network-sites:
            # renovate: depName="UniFi-Poller: Network Sites - Prometheus"
            gnetId: 11311
            revision: 5
            datasource: Prometheus
          unifi-uap:
            # renovate: depName="UniFi-Poller: UAP Insights - Prometheus"
            gnetId: 11314
            revision: 10
            datasource: Prometheus
          unifi-usw:
            # renovate: depName="UniFi-Poller: USW Insights - Prometheus"
            gnetId: 11312
            revision: 9
            datasource: Prometheus
        ceph:
          ceph-cluster:
            # renovate: depName="Ceph Cluster"
            gnetId: 2842
            revision: 17
            datasource: Prometheus
          ceph-osd:
            # renovate: depName="Ceph - OSD (Single)"
            gnetId: 5336
            revision: 9
            datasource: Prometheus
          ceph-pools:
            # renovate: depName="Ceph - Pools"
            gnetId: 5342
            revision: 9
            datasource: Prometheus
        kubernetes:
          kubernetes-api-server:
            # renovate: depName="Kubernetes / System / API Server"
            gnetId: 15761
            revision: 14
            datasource: Prometheus
          kubernetes-coredns:
            # renovate: depName="Kubernetes / System / CoreDNS"
            gnetId: 15762
            revision: 13
            datasource: Prometheus
          kubernetes-global:
            # renovate: depName="Kubernetes / Views / Global"
            gnetId: 15757
            revision: 31
            datasource: Prometheus
          kubernetes-namespaces:
            # renovate: depName="Kubernetes / Views / Namespaces"
            gnetId: 15758
            revision: 27
            datasource: Prometheus
          kubernetes-nodes:
            # renovate: depName="Kubernetes / Views / Nodes"
            gnetId: 15759
            revision: 19
            datasource: Prometheus
          kubernetes-pods:
            # renovate: depName="Kubernetes / Views / Pods"
            gnetId: 15760
            revision: 22
            datasource: Prometheus
    prometheus:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Prometheus"
        hosts:
          - &host "prom.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host

      prometheusSpec:
        retentionSize: "40GiB"
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        ruleSelector: {}
        additionalScrapeConfigs:
          - job_name: firewall02-node
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall02.${SECRET_DOMAIN}:9100"
          - job_name: firewall02-haproxy
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall02.${SECRET_DOMAIN}:8404"
          - job_name: firewall03-node
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall03.${SECRET_DOMAIN}:9100"
          - job_name: firewall03-haproxy
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall03.${SECRET_DOMAIN}:8404"
          - job_name: unraid
            honor_timestamps: true
            static_configs:
              - targets:
                  - "${CORE_NFS_SERVER}:9100"
    alertmanager:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Alertmanager"
        hosts:
          - &host "alerts.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host
