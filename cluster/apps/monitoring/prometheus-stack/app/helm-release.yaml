---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: &app kube-prometheus-stack
spec:
  interval: 30m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 72.5.3
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
  values:
    kubeApiServer:
      enabled: true
      serviceMonitor:
        selector:
          k8s-app: kube-apiserver
        metricRelabelings:
          # Drop high cardinality labels by onedr0p
          - action: drop
            sourceLabels: ["__name__"]
            regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
          - action: drop
            sourceLabels: ["__name__"]
            regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
    kubeControllerManager: &kubeControllerManager
      service:
        selector:
          k8s-app: kube-controller-manager
    kubeEtcd:
      <<: *kubeControllerManager # etcd runs on control plane nodes
    kubeScheduler:
      serviceMonitor:
        selector:
          k8s-app: kube-scheduler
    kubeProxy:
      enabled: false
    coreDns:
      enabled: true
      serviceMonitor:
        enabled: true
        selector:
          k8s-app: kube-dns
    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          # Remove duplicate labels provided by k3s
          - action: keep
            sourceLabels: ["__name__"]
            regex: (apiserver_audit|apiserver_client|apiserver_delegated|apiserver_envelope|apiserver_storage|apiserver_webhooks|authentication_token|cadvisor_version|container_blkio|container_cpu|container_fs|container_last|container_memory|container_network|container_oom|container_processes|container|csi_operations|disabled_metric|get_token|go|hidden_metric|kubelet_certificate|kubelet_cgroup|kubelet_container|kubelet_containers|kubelet_cpu|kubelet_device|kubelet_graceful|kubelet_http|kubelet_lifecycle|kubelet_managed|kubelet_node|kubelet_pleg|kubelet_pod|kubelet_run|kubelet_running|kubelet_runtime|kubelet_server|kubelet_started|kubelet_volume|kubernetes_build|kubernetes_feature|machine_cpu|machine_memory|machine_nvm|machine_scrape|node_namespace|plugin_manager|prober_probe|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|storage_operation|volume_manager|volume_operation|workqueue)_(.+)
          - action: replace
            sourceLabels: ["node"]
            targetLabel: instance
          # Drop high cardinality labels
          - action: labeldrop
            regex: (uid)
          - action: labeldrop
            regex: (id|name)
          - action: drop
            sourceLabels: ["__name__"]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)
    grafana:
      assertNoLeakedSecrets: false
      adminPassword: ${GRAFANA_PASS}
      defaultDashboardsTimezone: browser
      deploymentStrategy:
        type: Recreate
      plugins:
        - grafana-piechart-panel
      serviceMonitor:
        enabled: true
      persistence:
        type: pvc
        enabled: true
        storageClassName: ceph-block
        accessModes:
          - ReadWriteOnce
        size: 2Gi
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Grafana"
        hosts:
          - &host "grafana.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host
      grafana.ini:
        server:
          root_url: "https://grafana.home.${SECRET_DOMAIN}"
        analytics:
          check_for_updates: false
        security:
          allow_embedding: true
        auth:
          signout_redirect_url: "https://auth.home.${SECRET_DOMAIN}/application/o/grafana/end-session/"
          # Switch off auto_login to be able to log in as admin to solve https://github.com/grafana/grafana/issues/29211
          oauth_auto_login: false
        auth.generic_oauth:
          enabled: true
          name: "Authentik"
          client_id: "${GRAFANA_OAUTH_CLIENTID}"
          client_secret: "${GRAFANA_OAUTH_SECRET}"
          scopes: "openid profile email"
          auth_url: "https://auth.${SECRET_DOMAIN}/application/o/authorize/"
          token_url: "https://auth.${SECRET_DOMAIN}/application/o/token/"
          api_url: "https://auth.${SECRET_DOMAIN}/application/o/userinfo/"
          allow_sign_up: true
          email_attribute_name: email

        auth.generic_oauth.group_mapping:
          role_attribute_path: |
            contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'people') && 'Viewer'
          org_id: 1
        auth.basic:
          enabled: false
          # disable_login_form: false
        auth.anonymous:
          enabled: false

      datasources:
        datasource.yaml:
          apiVersion: 1
          deleteDatasources:
            - name: Loki
              orgId: 1
            - name: Alertmanager
              orgId: 1
            - name: Prometheus
              orgId: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://kube-prometheus-stack-prometheus.monitoring:9090
              isDefault: true
              uid: Prometheus
            - name: Loki
              type: loki
              access: proxy
              url: http://loki-stack:3100
            - name: Alertmanager
              type: alertmanager
              access: proxy
              url: http://kube-prometheus-stack-alertmanager.monitoring:9093
              jsonData:
                implementation: prometheus
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: "default"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
            - name: "unifi"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/unifi
            - name: "ceph"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/ceph
            - name: "kubernetes"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/kubernetes
      dashboards:
        default:
          spegel:
            # renovate: depName="Spegel"
            gnetId: 18089
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          cloudflared:
            # renovate: depName="Cloudflare Tunnels (cloudflared)"
            gnetId: 17457 # https://grafana.com/grafana/dashboards/17457?tab=revisions
            revision: 6
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/13502-minio-dashboard/
          minio:
            gnetId: 13502
            revision: 19
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/1860-node-exporter-full/
          node-exporter-full:
            gnetId: 1860
            revision: 29
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/763-redis-dashboard-for-prometheus-redis-exporter-1-x/
          redis:
            gnetId: 763
            revision: 4
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/5342-ceph-pools/
          cert-manager:
            url: https://raw.githubusercontent.com/monitoring-mixins/website/refs/heads/master/assets/cert-manager/dashboards/overview.json
            datasource: Prometheus
          flux-cluster:
            url: https://raw.githubusercontent.com/fluxcd/flux2-monitoring-example/refs/heads/main/monitoring/configs/dashboards/cluster.json
            datasource: Prometheus
          flux-control-plane:
            url: https://raw.githubusercontent.com/fluxcd/flux2-monitoring-example/refs/heads/main/monitoring/configs/dashboards/control-plane.json
            datasource: Prometheus
          flux-logs:
            url: https://raw.githubusercontent.com/fluxcd/flux2-monitoring-example/refs/heads/main/monitoring/configs/dashboards/logs.json
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/21356-volsync-dashboard/
          volsync:
            gnetId: 21356
            revision: 3
            datasource: Prometheus
        unifi:
          unifi-insights:
            # renovate: depName="UniFi-Poller: Client Insights - Prometheus"
            gnetId: 11315
            revision: 9
            datasource: Prometheus
          unifi-network-sites:
            # renovate: depName="UniFi-Poller: Network Sites - Prometheus"
            gnetId: 11311
            revision: 5
            datasource: Prometheus
          unifi-uap:
            # renovate: depName="UniFi-Poller: UAP Insights - Prometheus"
            gnetId: 11314
            revision: 10
            datasource: Prometheus
          unifi-usw:
            # renovate: depName="UniFi-Poller: USW Insights - Prometheus"
            gnetId: 11312
            revision: 9
            datasource: Prometheus
        ceph:
          ceph-cluster:
            # renovate: depName="Ceph Cluster"
            gnetId: 2842
            revision: 17
            datasource: Prometheus
          ceph-osd:
            # renovate: depName="Ceph - OSD (Single)"
            gnetId: 5336
            revision: 9
            datasource: Prometheus
          ceph-pools:
            # renovate: depName="Ceph - Pools"
            gnetId: 5342
            revision: 9
            datasource: Prometheus
        kubernetes:
          kubernetes-api-server:
            # renovate: depName="Kubernetes / System / API Server"
            gnetId: 15761
            revision: 14
            datasource: Prometheus
          kubernetes-coredns:
            # renovate: depName="Kubernetes / System / CoreDNS"
            gnetId: 15762
            revision: 13
            datasource: Prometheus
          kubernetes-global:
            # renovate: depName="Kubernetes / Views / Global"
            gnetId: 15757
            revision: 31
            datasource: Prometheus
          kubernetes-namespaces:
            # renovate: depName="Kubernetes / Views / Namespaces"
            gnetId: 15758
            revision: 27
            datasource: Prometheus
          kubernetes-nodes:
            # renovate: depName="Kubernetes / Views / Nodes"
            gnetId: 15759
            revision: 19
            datasource: Prometheus
          kubernetes-pods:
            # renovate: depName="Kubernetes / Views / Pods"
            gnetId: 15760
            revision: 22
            datasource: Prometheus
    cleanPrometheusOperatorObjectNames: true
    prometheus:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Prometheus"
        hosts:
          - &host "prom.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host

      prometheusSpec:
        retentionSize: "40GiB"
        serviceMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - memory-snapshot-on-shutdown
        serviceMonitorSelector: {}
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        podMonitorNamespaceSelector: {}
        podMonitorSelector: {}
        ruleNamespaceSelector: {}
        ruleSelector: {}
        additionalScrapeConfigs:
          - job_name: firewall02-node
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall02.${SECRET_DOMAIN}:9100"
          - job_name: firewall02-haproxy
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall02.${SECRET_DOMAIN}:8404"
          - job_name: firewall03-node
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall03.${SECRET_DOMAIN}:9100"
          - job_name: firewall03-haproxy
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall03.${SECRET_DOMAIN}:8404"
          - job_name: unraid
            honor_timestamps: true
            static_configs:
              - targets:
                  - "${CORE_NFS_SERVER}:9100"
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      metricLabelsAllowlist:
        - pods=[*]
        - deployments=[*]
        - persistentvolumeclaims=[*]
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
    alertmanager:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Alertmanager"
        hosts:
          - &host "alerts.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host
    additionalPrometheusRulesMap:
      dockerhub-rules:
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
