---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 45.31.1
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    kubeEtcd:
      enabled: true
      endpoints:
        - 10.0.1.41
        - 10.0.1.42
        - 10.0.1.43
        # - 10.0.1.45
      service:
        enabled: true
        port: 2381
        targetPort: 2381
      serviceMonitor:
        scheme: http
    kubeControllerManager:
      enabled: true
      service:
        enabled: true
        selector:
          k8s-app: kube-controller-manager
      serviceMonitor:
        scheme: https
    kubeScheduler:
      enabled: true
      service:
        enabled: true
        selector:
          k8s-app: kube-scheduler
      serviceMonitor:
        scheme: https
    kubeProxy:
      enabled: false
    coreDns:
      enabled: true
      service:
        enabled: true
        selector:
          k8s-app: kube-dns
    kubelet:
      enabled: true
      endpoints:
        - 10.0.1.41
        - 10.0.1.42
        - 10.0.1.43
        - 10.0.1.44
        - 10.0.1.45
      service:
        enabled: true
        port: 10250
        targetPort: 10250
    grafana:
      adminPassword: ${GRAFANA_PASS}
      defaultDashboardsTimezone: browser
      deploymentStrategy:
        type: Recreate
      plugins:
        - grafana-piechart-panel
      # dashboards:
      #   default:
      #     # Ref: https://grafana.com/grafana/dashboards/13502
      #     minio:
      #       gnetId: 13502
      #       revision: 19
      #       datasource: Prometheus
      serviceMonitor:
        enabled: true
      persistence:
        type: pvc
        enabled: true
        storageClassName: ceph-block
        accessModes:
          - ReadWriteOnce
        size: 2Gi
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Grafana"
        hosts:
          - &host "grafana.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host
      grafana.ini:
        server:
          root_url: "https://grafana.home.${SECRET_DOMAIN}"
        analytics:
          check_for_updates: false
        security:
          allow_embedding: true
        auth:
          signout_redirect_url: "https://auth.home.${SECRET_DOMAIN}/application/o/grafana/end-session/"
          # Switch off auto_login to be able to log in as admin to solve https://github.com/grafana/grafana/issues/29211
          oauth_auto_login: false
        auth.generic_oauth:
          enabled: true
          name: "Authentik"
          client_id: "${GRAFANA_OAUTH_CLIENTID}"
          client_secret: "${GRAFANA_OAUTH_SECRET}"
          scopes: "openid profile email"
          auth_url: "https://auth.${SECRET_DOMAIN}/application/o/authorize/"
          token_url: "https://auth.${SECRET_DOMAIN}/application/o/token/"
          api_url: "https://auth.${SECRET_DOMAIN}/application/o/userinfo/"
          allow_sign_up: true
          email_attribute_name: email

        auth.generic_oauth.group_mapping:
          role_attribute_path: |
            contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'people') && 'Viewer'
          org_id: 1
        auth.basic:
          enabled: false
          # disable_login_form: false
        auth.anonymous:
          enabled: true
          org_name: HomeOps
          org_id: 1
          org_role: Viewer

      datasources:
        datasource.yaml:
          apiVersion: 1
          deleteDatasources:
            - name: Loki
              orgId: 1
            - name: Alertmanager
              orgId: 1
            - name: Prometheus
              orgId: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://kube-prometheus-stack-prometheus.monitoring:9090
              isDefault: true
              uid: Prometheus
            - name: Loki
              type: loki
              access: proxy
              url: http://loki-stack:3100
            - name: Alertmanager
              type: alertmanager
              access: proxy
              url: http://kube-prometheus-stack-alertmanager.monitoring:9093
              jsonData:
                implementation: prometheus
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: "default"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          # # Ref: https://grafana.com/grafana/dashboards/7845-zfs/
          # zfs:
          #   gnetId: 7845
          #   revision: 4
          #   datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/13502-minio-dashboard/
          minio:
            gnetId: 13502
            revision: 19
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/1860-node-exporter-full/
          node-exporter-full:
            gnetId: 1860
            revision: 29
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/763-redis-dashboard-for-prometheus-redis-exporter-1-x/
          redis:
            gnetId: 763
            revision: 4
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/5342-ceph-pools/
          ceph-pools:
            gnetId: 5342
            revision: 9
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/5336-ceph-osd-single/
          ceph-osd:
            gnetId: 5336
            revision: 9
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/2842-ceph-cluster/
          ceph-cluster:
            gnetId: 2842
            revision: 16
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11311-unifi-poller-network-sites-prometheus/
          unifi-poller-sites:
            gnetId: 11311
            revision: 5
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11315-unifi-poller-client-insights-prometheus/
          unifi-poller-clients:
            gnetId: 11315
            revision: 9
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11314-unifi-poller-uap-insights-prometheus/
          unifi-poller-uap:
            gnetId: 11314
            revision: 10
            datasource: Prometheus
          cert-manager:
            url: https://raw.githubusercontent.com/monitoring-mixins/website/master/assets/cert-manager/dashboards/cert-manager.json
            datasource: Prometheus
          flux-cluster:
            url: https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/monitoring-config/dashboards/cluster.json
            datasource: Prometheus
          flux-control-plane:
            url: https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/monitoring-config/dashboards/control-plane.json
            datasource: Prometheus
          flux-logs:
            url: https://raw.githubusercontent.com/fluxcd/flux2/main/manifests/monitoring/monitoring-config/dashboards/logs.json
            datasource: Prometheus
          kubernetes-api-server:
            url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json
            datasource: Prometheus
          kubernetes-coredns:
            url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json
            datasource: Prometheus
          kubernetes-global:
            url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
            datasource: Prometheus
          kubernetes-namespaces:
            url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
            datasource: Prometheus
          kubernetes-nodes:
            url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
            datasource: Prometheus
          kubernetes-pods:
            url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
            datasource: Prometheus
    prometheus:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Prometheus"
        hosts:
          - &host "prom.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host

      prometheusSpec:
        retentionSize: "35GiB"
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 40Gi
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        ruleSelector: {}
        additionalScrapeConfigs:
          - job_name: firewall-node
            honor_timestamps: true
            static_configs:
              - targets:
                  - "firewall.${SECRET_DOMAIN}:9100"
    alertmanager:
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-rfc1918-ips@kubernetescrd"
          hajimari.io/enable: "true"
          hajimari.io/appName: "Alertmanager"
        hosts:
          - &host "alerts.home.${SECRET_DOMAIN}"
        tls:
          - secretName: "wildcard-internal-${SECRET_DOMAIN/./-}-tls"
            hosts:
              - *host
